[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AP Stats Review",
    "section": "",
    "text": "Preface\nThese are draft notes intended to support review for AP Statistics. They are a work in progress and may change frequently as material is added or revised. They have not been fully proofread and are provided as is, without any guarantees regarding completeness or accuracy.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "one-variable-data.html",
    "href": "one-variable-data.html",
    "title": "1  One varialbe data",
    "section": "",
    "text": "1.1 The normal distribution\nThe normal distribution, also known as the bell curve and as the Gaussian distribution, is one of the most famous mathematical concepts in history.\nOne reason for this is that approximately normal distributions occur in many situations, including gambling winnings, heights, weights, blood pressure, standardized test scores, and experimental measurement errors.\nThis is not for discrete random variables but continuous random variables. If \\(X\\) has a normal pdf it can take on any continuous value. Therefore \\(Pr(X=x)\\) does not make sense anymore.\nThe normal distribution is defined with a mathematical formula. For any interval \\((a,b)\\), the proportion of values in that interval can be computed using this formula:\n\\[\\mbox{Pr}(a &lt; x \\leq b) = \\int_a^b \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{1}{2}\\left( \\frac{x-\\mu}{\\sigma} \\right)^2} \\, dx\\]\nYou don’t need to memorize the formula.\nThe most important characteristics is that it is completely defined by just two parameters: \\(\\mu\\) and \\(\\sigma\\). The rest of the symbols in the formula represent the interval ends, \\(a\\) and \\(b\\), and known mathematical constants \\(\\pi\\) and \\(e\\).\nThese two parameters, \\(\\mu\\) and \\(\\sigma\\), are referred to as the mean and the standard deviation (SD) of the distribution, respectively.\nThe distribution is symmetric, centered at \\(\\mu\\), and most values (about 95%) are within \\(2\\sigma\\) from \\(\\mu\\). Here is what the normal distribution looks like when the \\(\\mu = 0\\) and \\(\\sigma = 1\\):\nIf \\(\\mu = 0\\) and \\(\\sigma = 1\\) it is called standard.\nMemorize the following: For a standard normal random variable \\(Z\\):\nYou can use the normalcdf function to obtain these probabilities\nAlso memorize:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>One varialbe data</span>"
    ]
  },
  {
    "objectID": "one-variable-data.html#the-normal-distribution",
    "href": "one-variable-data.html#the-normal-distribution",
    "title": "1  One varialbe data",
    "section": "",
    "text": "68% are between -1 and 1\n95% are between -2 and 2\n99.7% between -3 and 3\n\n\n\n\\(\\text{P}(a &lt; Z &lt; b)\\) is normalcdf(a,b,0,1), 0 and 1 are mean and SD, respectively\n\\(\\text{P}(Z &lt; a)\\) is normalcdf(-1E99,a,0,1). -1E99 is \\(-\\infty\\)\n\n\n\nIf \\(Z\\) is normal and \\(a\\) is a constant \\(aZ\\) is normal.\nIf \\(Z\\) is normal and \\(b\\) is a constant \\(Z+b\\) is normal.\nIf \\(Z\\) is normal and \\(a,b\\) are a constant \\(aZ+b\\) is normal.\nIf \\(Z\\) and \\(W\\) are normal and \\(Z+W\\) is normal.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>One varialbe data</span>"
    ]
  },
  {
    "objectID": "random-variables.html",
    "href": "random-variables.html",
    "title": "2  Random Variables and Probability Distributions",
    "section": "",
    "text": "2.1 Discrete random variables\nThe pdf is often shown as a table or a graph. For the graph we simply plot a bar for each \\(k\\) going up to \\(p_k\\).\nHere is th pdf for the sum of two dice:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Random Variables and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "random-variables.html#discrete-random-variables",
    "href": "random-variables.html#discrete-random-variables",
    "title": "2  Random Variables and Probability Distributions",
    "section": "",
    "text": "Discrete random variable\n\n\n\n\\(X\\) can have different outcomes, each one with a probability. Discrete means the possible outcomes are finite.\n\n\n\n\n\n\n\n\nProbability density function (pdf)\n\n\n\nDefines the probability \\(\\text{P}(X = k)\\) for each outcome \\(k\\)\nThe exam often uses short hand \\(p_k = \\text{P}(X=k)\\)\n\n\n\n\n\n\n\n\nNote\n\n\n\nLater we will need the cumulative distribution function which is simply defined by\n\\[\n\\begin{aligned}\nF(a) &= \\text{P}(X &lt;= a)\\\\\n&= \\sum_{x_i \\leq a} \\text{P}(X=x_i)\n\\end{aligned}\n\\]\nInterpretation \\(F(a)\\) tells us the probability of \\(X\\) being less than \\(a\\) for any \\(a\\).\n\n\n\n\n\n\n\n\nExample 1: Fair coin\n\n\n\nDefine \\(X=0\\) for tails and \\(X=1\\) for heads\n\\[\n\\text{P}(X=0) = 1/2\\\\\n\\text{P}(X=1) = 1/2\n\\]\n\n\n\n\n\n\n\n\nExample 2: A die\n\n\n\n\\[\n\\text{P}(X=1) = 1/6\\\\\n\\text{P}(X=2) = 1/6\\\\\n\\vdots\\\\\n\\text{P}(X=6) = 1/6\n\\]\n\n\n\n\n\n\n\n\nExample 3: Sum of two dice\n\n\n\n\\[\n\\text{P}(X=2)  = 1/36, \\\\\n\\text{P}(X=3)  = 2/36,\n\\\\\n\\vdots\n\\\\\\text{P}(X=7) = 1/6\n\\\\\n\\vdots\\\\\n\\text{P}(X=12)  = 1/36\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Random Variables and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "random-variables.html#mean-and-standard-deviation-of-a-random-variable",
    "href": "random-variables.html#mean-and-standard-deviation-of-a-random-variable",
    "title": "2  Random Variables and Probability Distributions",
    "section": "2.2 Mean and standard deviation of a random variable",
    "text": "2.2 Mean and standard deviation of a random variable\nThe mean and standard deviation of the the distribution of a random variable \\(X\\) are referred as the mean and standard deviation of \\(X\\).\n\n\n\n\n\n\nMean\n\n\n\n\\[\n\\mu_X = \\sum_{i=1}^n x_i \\, \\text{P}(X=x_i)\n\\]\nWith \\[x_1, \\dots, x_n\\] all the possible outcomes.\n\n\nFor any random variable \\(*\\) we use the symbol \\(\\mu_{*}\\) to represent it’s mean. It can be any random variable and we don’t always use the name \\(X\\).\n\n\n\n\n\n\nThe standard deviation (SD)\n\n\n\nThe standard de deviation of the distribution of a random variable \\(X\\) is defined as:\n\\[\n\\sigma_X = \\sqrt{\\sum_{i=1}^n (x_i-\\mu_X)^2 \\, \\text{P}(X=x_i)}\n\\]\n\n\nWe use \\(\\sigma_{*}\\) just like \\(\\mu_{*}\\)\nYou can think of this as the typical distance you see \\(X\\) from the mean \\(\\mu_X\\). For the heads or tails this is 1/2 since both 0 and 1 are1/2 from the mean 1/2.\n\n\n\n\n\n\nNote\n\n\n\nWe sometimes say the standard error of \\(X\\) to mean the standard deviation of the distribution of a random variable \\(X\\).\n\n\n\n\n\n\n\n\nThe variance\n\n\n\nThe variance is simply the standard deviation squared \\(\\sigma_X^2\\).\n\nWe define the variance because mathematical calculations are easier without the square root. Other than that we always use standard deviation.\nThe standard deviation has the same units as \\(X\\). The variance has units squared which has no interpretation. What is kilograms squared or dollars squared?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Random Variables and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "random-variables.html#bernoulli-trials",
    "href": "random-variables.html#bernoulli-trials",
    "title": "2  Random Variables and Probability Distributions",
    "section": "2.3 Bernoulli trials",
    "text": "2.3 Bernoulli trials\n\nA super common example of a useful random variable are Bernoulli trials.\nThese are either a 0 (failure) or 1 (success) and each trial is independent of of others.\n\n\n\n\n\n\n\nBernoulli trial definition\n\n\n\n\n\\(X\\) is either 1 (success) or failure (0).\nCompletely defined by the probability of success: \\(\\text{P}(X=1) = p\\).\nThe probability of failure is simply \\(1-p\\), sometimes called \\(q\\).\n\n\n\nBernoulli trials are popular because we can use them to count random things: number of heads when we toss coins, number of lottery winners, number of defective light bulbs made in a day by a factory, number of patients that got cured by a drug, number of COVID-19 hospitalizations in a day, and so on.\n\n\n\n\n\n\nBernoulli trial mean and SD\n\n\n\nIf \\(X\\) is a Bernoulli trial:\n\n\\(\\mu_X = p\\)\n\\(\\sigma_X = \\sqrt{p(1-p)}\\)\n\n\n\nYou need to memorize this but here is the derivation\n\\[\n\\begin{aligned}\n\\mu_X &= 0 \\times \\text{P}(X=0) + 1 \\times \\text{P}(X=1)\\\\\n&= p\n\\end{aligned}\n\\]\nand\n\\[\n\\begin{aligned}\n\\sigma_X^2 &= (0 - p)^2 (1-p) + (1-p)^2 p\\\\\n&= (1-p)p(p+1-p)\\\\\n&= p(1-p)\n\\end{aligned}\n\\]\n\n\n\n\n\n\nExamples\n\n\n\n\nTossing coins, \\(p=0.5\\)\nSteph Curry free throws, \\(p=0.9\\)\nLottery winners, \\(p &lt; 10^{-6}\\)\nCeltics win a game in NBA finals \\(p = ?\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Random Variables and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "random-variables.html#combining-shifting-and-scaling-random-variables",
    "href": "random-variables.html#combining-shifting-and-scaling-random-variables",
    "title": "2  Random Variables and Probability Distributions",
    "section": "2.4 Combining, shifting, and scaling random variables",
    "text": "2.4 Combining, shifting, and scaling random variables\n\n\n\n\n\n\nMean of linear combinations\n\n\n\nNeed to memorize these (they are intuitive). If \\(X\\) and \\(Y\\) random variables and \\(a\\) is a constant:\n\n\\(\\mu_{X+Y} = \\mu_{X}+ \\mu_{Y}\\)\n\\(\\mu_{X+a} = \\mu_{X} + a\\)\n\\(\\mu_{aX} = a \\mu_{X}\\)\n\n\n\n\n\n\n\n\n\nExample\n\n\n\nIf \\(X\\) and \\(Y\\) are two random variables, what is \\(\\mu_{X-Y}\\)?\n\\[\n\\begin{aligned}\n\\mu_{X-Y} &= \\mu_{X} + \\mu_{-Y}\\\\\n&= \\mu_{X}+ -1\\mu_{Y}\\\\\n&= \\mu_{X} - \\mu_{Y}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\nSD of linear combinations\n\n\n\nFor these we use the variance. But you can take square root at the end.\n\n\\(\\sigma^2_{X+a} = \\sigma^2_{X}\\): shifting does not change variability.\n\\(\\sigma^2_{aX} = a^2\\sigma^2_{X} \\implies \\sigma_{aX} = |a|\\sigma_{X}\\): change of scale also scales measure of variability.\nIf \\(X\\) and \\(Y\\) are independent, \\(\\sigma^2_{X+Y} = \\sigma_X^2 + \\sigma_Y^2 \\implies \\sigma_{X+Y} = \\sqrt{\\sigma_X^2 + \\sigma_Y^2}\\): Adding two things that vary, varies more.\n\n\n\n\n\n\n\n\n\nExample\n\n\n\nIf \\(X\\) and \\(Y\\) are two independent random variables, what is \\(\\sigma_{X-Y}\\)?\n\\[\n\\begin{aligned}\n\\sigma^2_{X-Y} &= \\sigma^2_{X} + \\sigma^2_{-Y}\\\\\n&= \\sigma^2_{X}+ (-1)^2\\sigma_{Y}\\\\\n&= \\sigma^2_{X} + \\sigma^2_{Y}\n\\end{aligned}\n\\]\nWhich implies\n\\[\n\\sigma_{X-Y} = \\sqrt{\\sigma^2_{X} + \\sigma^2_{-Y}}\n\\]\nInterpretation: Subtracting two variables that vary independently has more variability than each.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Random Variables and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "random-variables.html#binomial-distribution",
    "href": "random-variables.html#binomial-distribution",
    "title": "2  Random Variables and Probability Distributions",
    "section": "2.5 Binomial distribution",
    "text": "2.5 Binomial distribution\nAnother popular random variable is the sum of Bernoulli trials.\n\\[\nS = \\sum_{i=1}^n X_i\n\\]\nIt tells us the number of successes and it is also a random variable.\nExamples:\n\nNumber of heads if I toss coins\nNumber of free throws curry makes\n\n\n\n\n\n\n\nExample\n\n\n\nWhat is \\(\\mu_S\\)?\n\\[\n\\begin{aligned}\n\\mu_S &= \\mu_{X_1+\\dots+X_n}\\\\\n&= \\mu_{X_1}+\\dots+\\mu_{X_n}\\\\\n&= np\n\\end{aligned}\n\\]\nWhat is \\(\\sigma_S\\)?\n\\[\n\\begin{aligned}\n\\sigma^2_S &= \\sigma^2_{X_1+\\dots+X_n}\\\\\n&= \\sigma^2_{X_1}+\\dots+\\sigma^2_{X_n}\\\\\n&= np(1-p)\n\\end{aligned}\n\\] This implies\n\\[\n\\sigma_S=\\sqrt{np(1-p)}\n\\]\n\n\n\n\n\n\n\n\nBinomial pdf\n\n\n\nWe can compute the pdf for the sum of \\(n\\) trials:\n\\[\n\\text{P}(S = k)  = \\binom{n}{k} p^k (1-p)^{n-k}\n\\]\nThis is called the binomial distribution and can be computed in AP-test with binompdf(n, p, k) and the CDF with binomcdf(n,p,k)\n\n\nThe CDF is useful for answering questions such as “what is the chance that we see 3 heads or less?” or “what is the chance we see 4,5,6 heads?”\n\n\n\n\n\n\nExample\n\n\n\npdf of the number of heads when tossing 10 coins:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Random Variables and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "random-variables.html#geometric-distribution",
    "href": "random-variables.html#geometric-distribution",
    "title": "2  Random Variables and Probability Distributions",
    "section": "2.6 Geometric distribution",
    "text": "2.6 Geometric distribution\nIt is also common to ask how many trials do I need to see a success. For example, how many free throws will Curry take until he misses.\n\n\n\n\n\n\nGeometric distribution\n\n\n\nDefine a random variable as \\(X\\)=number of trials if we stop after the first success.\nIt is not hard to see that this is: \\[\n\\text{P}(X=k) = (1-p)^k p\n\\]\nThis is called the Geometric distribution, defined for \\(k=1,2,\\dots,\\infty\\).\n\n\n\n\n\n\n\n\nExample: number of free thows before Curry misses.\n\n\n\nHere miss is the success we are waiting for so \\(p=0.1\\)\n\n\n\n\n\n\n\n\n\nWe use this to calculate, for example, that the chance of seeing 10 or more free throws in a row to start the game is 1 - geomcdf(10, .1) = 0.3138106",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Random Variables and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "random-variables.html#approximation-to-binomial",
    "href": "random-variables.html#approximation-to-binomial",
    "title": "2  Random Variables and Probability Distributions",
    "section": "2.7 Approximation to Binomial",
    "text": "2.7 Approximation to Binomial\nWhen the number of trials is large binomial is very well approximated by the normal distribution.\nDefine\n\\[\nZ = \\frac{S - np}{\\sqrt{np(1-p)}}\n\\]\nThen Z is approximated by standard normal\nHere is a \\(n=10, p=0.5\\) binomial with a normal with mean \\(np = 5\\) and standard deviation \\(\\sqrt{np(1-p)} \\approx 1.6\\) added in blue\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nHere it is for 100. In this case \\(np = 50\\) and \\(\\sqrt{np(1-p)} = 5\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\nIf I toss 100 coins, what is the probability that I see between 45 and 55 heads?\nWe can use the binomial to answer this exactly binomcdf(100, 0.5, 55) - binomcdf(100, 0.5, 44) which is 0.728747.\nBut we can also use the normal distribution:\n\\[\n\\begin{aligned}\n\\text{P}(45 \\leq S \\leq 55) &= \\text{P}(44.5 &lt; S &lt; 55.5)\\\\\n&= \\text{P}(44.5 - 50 &lt; S - 50 &lt; 55.5-50)\\\\\n&= \\text{P}\\left(\\frac{44.5 - 50}{\\sqrt{100\\times 0.5\\times 0.5}} &lt; \\frac{S - 50}{\\sqrt{100\\times 0.5\\times 0.5}}&lt; \\frac{55.5-50}{\\sqrt{100\\times 0.5\\times 0.5}}\\right) \\\\\n&= \\text{P}(-1.1 &lt; Z &lt; 1.1 )\n\\end{aligned}\n\\]\nWe use normalcdf(-1.1 1.1, 0, 1) = 0.7286679\nWhich is almost identical to the binomial result.\n\n\n\n\n\n\n\n\nNote\n\n\n\nImportant to understand why we to the first \\(\\text{P}(45 \\leq S \\leq 55) = \\text{P}(44.5 &lt; S &lt; 55.5)\\). The normal distribution is continuous so it can’t be equal to anything. So we do the adjustment to make sure we include 45 and 55 in the approximation.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Random Variables and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "sampling-distributions.html",
    "href": "sampling-distributions.html",
    "title": "3  Sampling distributions",
    "section": "",
    "text": "3.1 Population and parameters",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling distributions</span>"
    ]
  },
  {
    "objectID": "sampling-distributions.html#population-and-parameters",
    "href": "sampling-distributions.html#population-and-parameters",
    "title": "3  Sampling distributions",
    "section": "",
    "text": "Population\n\n\n\nThe population is defined by the list of numbers \\(x_1, x_2, \\dots, x_n\\).\nThe \\(x\\)s are not random.\nWe can’t see the entire population but we want to learn about it.\n\n\n\n\n\n\n\n\nExample 1: Trump voters\n\n\n\nThe population is the people who will vote on election day. Trump voters get a 1 and others get a 0. So all the \\(x\\)s are either 0 or 1.\n\n\n\n\n\n\n\n\nExample 2: High school SAT scores\n\n\n\nThe population are all the students that took SAT. The \\(x\\)s are the scores for each student.\n\n\n\n\n\n\n\n\nPopulation parameters\n\n\n\nThe population parameters are summaries of the \\(x_1, x_2, \\dots, x_n\\) we are interested in.\nIn the AP test we almost always care about the population mean:\n\\[\n\\mu = \\frac{1}{n}\\sum_{i=1}^n x_i\n\\]\nMost of this chapter is about estimating the population mean \\(\\mu\\).\nAnother parameter we will need is the population standard deviation:\n\\[\n\\sigma = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (x_i-\\mu)^2}\n\\]\n\n\n\n\n\n\n\n\nPopulation proportion\n\n\n\nWhen the \\(x\\)s are 0s or 1s, then the population mean is equivalent to the proportion of 1s.\nIn this case we use the symbol \\(p\\) instead of \\(\\mu\\).\nAnd the standard deviation can be shown to be \\(\\sqrt{p(1-p)}\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling distributions</span>"
    ]
  },
  {
    "objectID": "sampling-distributions.html#the-sample-average",
    "href": "sampling-distributions.html#the-sample-average",
    "title": "3  Sampling distributions",
    "section": "3.2 The sample average",
    "text": "3.2 The sample average\nThe strategy to estimate the population parameter is to take a random sample: we can’t examine all the sample so we examine a much smaller subset.\nWe learn that we can learn a lot about population parameters from samples.\nBy far the most common example is using a sample average to estimate a population mean.\n\n\n\n\n\n\nA sample\n\n\n\n\nA sample are the resulting observed values we obtain when picking individuals at random from the population.\nWe represent them with capital letters because they are random variables:\n\n\\[X_1, \\dots, X_N\\]\n\n\n\n\n\n\n\n\nThe sample size\n\n\n\n\\(N\\) is called the sample size.\nDo not confuse it with the number of individuals in the population \\(n\\).\nIn the election poll example \\(n\\) is over 100 million while a typical sample size \\(N\\) is 1,000 or less.\n\n\n\n\n\n\n\n\nThe sample average\n\n\n\n\nA sample are the resulting observed values we obtain when picking individuals at random from the population.\nWe represent them with capital letters because they are random variables:\n\n\\[X_1, \\dots, X_N\\] * \\(N\\) is called the sample size.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling distributions</span>"
    ]
  },
  {
    "objectID": "sampling-distributions.html#central-limit-theorem",
    "href": "sampling-distributions.html#central-limit-theorem",
    "title": "3  Sampling distributions",
    "section": "3.3 Central Limit Theorem",
    "text": "3.3 Central Limit Theorem\ntldr: The distribution of the sample average is approximated by a normal distribution when the sample size is large.\n\n\n\n\n\n\nCentral Limit Theorem (CLT)\n\n\n\n\nIf \\(X_1\\), \\(X_N\\) are random variables that are independent and have the same distribution, the sum \\(\\sum_{i=1}^N X_i\\) gets closer and closer to being normally distributed when \\(N\\) gets very large.\nBecause dividing a normal random variable by a constant is still normal, the CLT applies to the average \\(\\frac{1}{N}\\sum_{i=1}^N X_i\\) as well.\nRule of thumb \\(N\\geq 30\\) is considered large enough.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling distributions</span>"
    ]
  },
  {
    "objectID": "sampling-distributions.html#proportions",
    "href": "sampling-distributions.html#proportions",
    "title": "3  Sampling distributions",
    "section": "3.4 Proportions",
    "text": "3.4 Proportions\nA very common application of statistics is estimating a population proportion\nExamples:\n\nProportion of voters voting for trump.\nProportion of patients that a drug cures.\nProportion of adults with a job.\n\nWe want to to estimate \\(p\\), the population parameter.\nNote that:\n\nEach \\(X\\) in the sample is a Bernoulli trial because \\(\\text{P}(X=1) = p\\).\nThis implies that for all \\(i\\), \\(\\mu_{X_i} = p\\) and \\(\\sigma_{X_i} = \\sqrt{p(1-p)}\\)\nBecause we sample with replacement the \\(X\\)s are independent.\n\n\n\n\n\n\n\nMean and SD of sample proportion\n\n\n\nThe sample proportion is\n\\[\n\\hat{p} = \\frac{1}{N}\\sum_{i=1}^N X_i\n\\]\nUsing what we have learned about mean and SD of combinations and re-scaling we have:\n\n\\(\\mu_{\\hat{p}} = p\\)\n\\(\\sigma_{\\hat{p}} = \\frac{\\sigma_X}{\\sqrt{N}} = \\frac{\\sqrt{p(1-p)}}{\\sqrt{N}}\\)\n\n\n\n\n\n\n\n\n\nDistribution of sample proportion\n\n\n\n\n\\(\\hat{p}\\) is a sum of Bernoulli trials divided by a constant. So we could use the Binomial distribution to compute \\(\\text{P}(\\hat{p} = k/N)\\)\nHowever, in the exam they want you to use the CLT.\n\\(\\hat{p}\\) is approximated by normal distribution with mean \\(p\\) and SD \\(\\sqrt{p(1-p)/n}\\)\n\n\n\n\n\n\n\n\n\nExample\n\n\n\nIf I take a poll if 1000 people to get an idea of how many people are voting for Trump, what is the chance that my sample proportion \\(\\hat{p}=0.45\\) is within 1% of the actual proportion?\nWe are asking \\(\\text{P}(|\\hat{p} - p| &lt; 0.01)\\)\nLet’s figure it out:\n\\[\n\\begin{aligned}\n\\text{P}(|\\hat{p} - p| &lt; 0.01) &= \\text{P}(-0.01 &lt; \\hat{p} - p &lt; 0.01)\\\\\n&=\\text{P}\\left(\\frac{-0.01}{\\sqrt{\\frac{p(1-p)}{N}}}&lt; \\frac{\\hat{p} - p}{{\\sqrt{\\frac{p(1-p)}{N}}}} &lt; \\frac{0.01}{{\\sqrt{\\frac{p(1-p)}{N}}}}\\right)\\\\\n&=\\text{P}\\left(\\sqrt{1000}\\frac{-0.01}{\\sqrt{p(1-p)}}&lt; Z &lt; \\sqrt{1000}\\frac{0.01}{\\sqrt{p(1-p)}}\\right)\n\\end{aligned}\n\\]\nI don’t know \\(p\\) but in the exam they want you to stick in \\(\\hat{p}\\) for the SD calculation. So \\(\\sqrt{1000/(0.45\\times .55)}\\approx 63.5\\)\nso we have \\(\\text{P}(0.645 &lt; Z &lt; 0.645)\\) or normcdf(-0.645, 0.645, 0, 1) which is r pnorm(0.645)-pnorm(-0.645)\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn the exam compute the standard deviation \\(\\sqrt{p(1-p)/N}\\) first and stick that in the calculations instead of the formula.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling distributions</span>"
    ]
  },
  {
    "objectID": "sampling-distributions.html#means",
    "href": "sampling-distributions.html#means",
    "title": "3  Sampling distributions",
    "section": "3.5 Means",
    "text": "3.5 Means\nAnother common application of statistics is estimating a population mean\nExamples:\n\nWhat is the average SAT score in a high school?\nWhat is the average blood pressure for people taking a drug?\n\nWe want to to estimate \\(\\mu\\), the population parameter.\nNote that\n\nEach \\(X\\) in the sample has the same distribution \\(\\text{P}(X=x_i) = 1/n\\) for all \\(i\\).\nThis implies that for all \\(i\\), \\(\\mu_{X_i} = \\mu\\) and \\(\\sigma_{X_i} = \\sigma\\)\nBecause we sample with replacement the \\(X\\)s are independent.\n\n\n\n\n\n\n\nMean and SD of sample average\n\n\n\nThe sample average is\n\\[\n\\bar{X} = \\frac{1}{N}\\sum_{i=1}^N X_i\n\\]\nUsing what we have learned about mean and SD of combinations and re-scaling we have:\n\n\\(\\mu_{\\bar{X}} = \\mu\\)\n\\(\\sigma_{\\bar{X}} = \\frac{\\sigma}{\\sqrt{N}}\\)\n\n\n\n\n\n\n\n\n\nDistribution of sample average\n\n\n\nCLT tells us that \\(\\bar{X}\\) is approximated by a normal distribution with mean \\(\\mu\\) and SD \\(\\sqrt{\\sigma/n}\\)\n\n\n\n\n\n\n\n\nSample standard deviation\n\n\n\nIf I want to make probability calculations I need to know \\(\\sigma_{\\bar{X}}\\), but I don’t know \\(sigma\\).\nFor proportions we used \\(\\sqrt{\\hat{p}(1-\\hat{p})}\\) as an approximation of the standard deviation \\(\\sqrt{p(1-p)}\\)\nBut when sample means are not based on Bernoulli trials, we can’t do that.\nInstead we use the sample standard deviation.\n\\[\ns = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^N(X-\\bar{X})^2}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling distributions</span>"
    ]
  },
  {
    "objectID": "proportions.html",
    "href": "proportions.html",
    "title": "4  Proportions",
    "section": "",
    "text": "4.1 Confidence intrval\nTask:\nSolution:\nOur proportion \\(\\hat{p}\\) follows a normal distribution with mean \\(p\\) and standard deviation \\(\\sqrt{p(1-p)/N} \\approx \\sqrt{0.45\\times 0.55/1000} \\approx 0.016\\)\nLets consider symmetric intervals with: \\([\\hat{p} - B, \\hat{p} + B]\\)\nWe want to find a margin of error \\(\\text{MOE}\\) such that:\n\\[\n\\text{P}(p \\in [\\hat{p} - \\text{MOE}, \\hat{p} + \\text{MOE}]) = 0.95\n\\]\nWe accomplish it by setting \\(\\text{MOE} = 2 \\sigma_{\\hat{p}} \\approx \\sqrt{\\hat{p}(1-\\hat{p})}/\\sqrt{N} \\approx 0.03\\)\n\\[\n\\begin{aligned}\n\\text{P}(p \\in [\\hat{p} - 2 \\sigma_{\\hat{p}}, \\hat{p} + 2 \\sigma_{\\hat{p}}]) &= \\text{P}(\\hat{p} -  2 \\sigma_{\\hat{p}} &lt; p &lt; \\hat{p} +  2 \\sigma_{\\hat{p}}]) \\\\\n&= \\text{P}(-  2 \\sigma_{\\hat{p}} &lt; \\hat{p} - p &lt; 2 \\sigma_{\\hat{p}}])\\\\\n&= \\text{P}\\left(-  2  &lt; \\frac{\\hat{p} - p}{\\sigma_{\\hat{p}}} &lt; 2\\right)\\\\\n&= \\text{P}(-2 &lt; Z &lt; 2)\\\\\n&=0.95\n\\end{aligned}\n\\]\nSo our interval is \\(0.45 \\pm 0.03\\).\nWe refer to the 0.03 as the margin of error (MOE).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Proportions</span>"
    ]
  },
  {
    "objectID": "proportions.html#confidence-intrval",
    "href": "proportions.html#confidence-intrval",
    "title": "4  Proportions",
    "section": "",
    "text": "We take a sample of 1,000 voters.\n45% of our respondents say they will vote for trump.\nProvide an interval with 95% of containing the true proportion \\(p\\)\n\n\n\n\n\n\n\n\n\n\nIf we want to be 99.7% sure we can use 3 instead of 2.\nIf we want to be 68% sure we can use 1 instead of 2.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Proportions</span>"
    ]
  },
  {
    "objectID": "proportions.html#critical-values",
    "href": "proportions.html#critical-values",
    "title": "4  Proportions",
    "section": "4.2 Critical values",
    "text": "4.2 Critical values\nWe already knew that using \\(2\\) would give us a 95% confidence interval.\nBut what if we didn’t know? Or if we wanted a 99% confidence interval?\nThe function invNorm will do this for us. The impute is the area to the left.\nSo to obtain 95% we need 0.5% to the left and 0..5% to the right.\nWe use invNorm(0.995) which gives us r qnorm(0.995)\nSo we multiply by \\(2.57\\) not \\(2\\) to get a 99% confidence interval.\nNote that to get exactly 95% we actually use invNorm(0.975) which is r qnorm(0.975), a little bit less than 2. In some books you will see 1.96 instead of 2.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Proportions</span>"
    ]
  },
  {
    "objectID": "proportions.html#p-values",
    "href": "proportions.html#p-values",
    "title": "4  Proportions",
    "section": "4.3 p-values",
    "text": "4.3 p-values\n\nWe want to know if a coin in biased.\nWe toss it 100 times and observe 60% heads.\n\nIs it biased or can this happen by chance?\nLet’s compute the probability of seeing \\(\\hat{p} = 0.6\\) or more extreme.\nNote that 0.4 is as extreme: we usually permit both directions.\nNull hypothesis: It is fair or \\(p=0.5\\)\nWe will reject if the p-value is 0.05 or smaller.\nThe p-value is the probability observing something as extreme as we did when the null hypothesis holds\n\\[\n\\begin{aligned}\n\\text{P}(|\\hat{p} - p| \\geq 0.1) &= 1 - \\text{P}(|\\hat{p} - p| &lt; 0.1)\\\\\n&=1 - \\text{P}\\left(\\left|\\frac{\\hat{p}-p}{\\sigma_{\\hat{p}}}\\right| &lt; \\frac{0.1}{\\sigma_{\\hat{p}}}\\right)\\\\\n&=1 - \\text{P}\\left(|Z| &lt; \\frac{0.1}{\\sigma_{\\hat{p}}}\\right)\n\\end{aligned}\n\\]\nWhen the null hypothesis holds, \\(\\sigma_{\\hat{p}} = \\sqrt{0.5\\times 0.5 / 100} = 0.05\\)\nSo the p-value is \\(1 - \\text{P}(|Z| &lt; 0.1/0.05 = 2)\\) which is a bit less than 0.05\nWe reject the null hypothesis.\n\n\n\n\n\n\nType of errors\n\n\n\n\nType I error is rejecting the null hypothesis when it is true. Example say the coin is biased when it was fair.\nType II error is failing to reject the null hypothesis when it is not true. Example saying the coin is fair when it was biased.\nPower is the 1 - probability of Type II error.\n\n\n\nTo help remember:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Proportions</span>"
    ]
  },
  {
    "objectID": "proportions.html#difference-of-two-proportion",
    "href": "proportions.html#difference-of-two-proportion",
    "title": "4  Proportions",
    "section": "4.4 Difference of two proportion",
    "text": "4.4 Difference of two proportion\n\n4.4.1 Confidence interval\n\nDoes drug work better than placebo?\nThe proportion of the populations are \\(p_1\\) and \\(p_2\\).\nFor both placebo and drugged populations we obtain sample means \\(\\hat{p}_1\\) and \\(\\hat{p}_2\\)\nThe sample sizes are \\(N_1\\) and \\(N_2\\)\nProvide a 95% confidence interval\n\nWe know the difference \\(\\hat{p}_1 - \\hat{p}_2\\) had the following mean and SD:\n\n\\(\\mu_{\\hat{p}_1 - \\hat{p}_2} = p_1 - p_2\\)\n\\(\\sigma_{\\hat{p}_1 - \\hat{p}_2} = \\sqrt{ \\frac{p_1(1-p_1)}{N_1} + \\frac{p_2(1-p_2)}{N_2}}\\)\n\nTo construct a 95% confidence interval we use \\(\\hat{p}_1 - \\hat{p}_2 \\pm \\sigma_{\\hat{p}_1 - \\hat{p}_2}\\)\nAs before we estimate\n\\[\n\\sigma_{\\hat{p}_1 - \\hat{p}_2} =  \\sqrt{ \\frac{\\hat{p}_1(1-\\hat{p}_1)}{N_1} + \\frac{\\hat{p}_2(1-\\hat{p}_2)}{N_2}}\n\\]\n\n\n4.4.2 p-value\n\nSuppose we have sample sizes of 25 and 100 for the the drug and placebo group respectively and\nwe observe \\(\\hat{p}_1 = 0.25\\) and \\(\\hat{p}_2 = 0.15\\)\nThe null is that there is no difference so \\(p_1 - p_2\\) or \\(p_1 = p_2 = p\\)\n\nUnder the null hypothesis we have\n\n\\(\\mu_{\\hat{p}_1 - \\hat{p}_2} = 0\\)\n\\(\\sigma_{\\hat{p}_1 - \\hat{p}_2} = \\sqrt{ \\frac{p(1-p)}{N_1} + \\frac{p(1-p)}{N_2}}\\)\n\nTo compute the p-value we need an estimate for \\(\\sigma_{\\hat{p}_1 - \\hat{p}_2}\\) which depends on \\(p\\)\nWe estimate \\(p\\) with pooled data:\n\\[\\frac{N_1\\hat{p}_1 + N_2\\hat{p}_2}{N_1+N_2} = 0.17\\]\nwhich means \\(\\sigma_{\\hat{p}_1 - \\hat{p}_2} \\approx \\sqrt{(0.17\\times 0.83)(1/25+1/100)} \\approx 0.08\\)\nWith this we can compute\n\\[\n\\begin{aligned}\n\\text{P}(|\\hat{p}_1 - \\hat{p}_2| \\geq 0.1) &= 1 - \\text{P}(|Z| &lt; 0.1/\\sigma_{\\hat{p}_1 - \\hat{p}_2})\\\\\n&= 1 - \\text{P}(|Z| &lt; 0.1/0.08) \\\\\n&\\approx 0.20\n\\end{aligned}\n\\]\nWe do not reject.\n\n\n4.4.3 Confidence interval and p-value connection\nYou can do the math and see that if a 95% confidence interval does not include the null hypothesis mean, then a p-value will be less than 0.05\nThe math:\nIf the null hypothesis says the mean is \\(p\\) and the observed \\(\\hat{p}\\) resulted in a p-value less than 0.05, we know:\n\\[\n\\left| \\frac{\\hat{p} - p}{\\sigma_{\\hat{p}}} \\right| &gt; 2\n\\] This implies that either\n\\[\np &gt; \\hat{p} + 2\\sigma_{\\hat{p}} \\text{ or } p &lt; \\hat{p} - 2\\sigma_{\\hat{p}}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Proportions</span>"
    ]
  },
  {
    "objectID": "means.html",
    "href": "means.html",
    "title": "5  Means",
    "section": "",
    "text": "5.1 Confidence intrval\nTask:\nSolution:\nThe sample average \\(\\bar{X}\\) follows a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma/\\sqrt{N} \\approx 204 / 6 = 34\\)\nAs with proportions we have \\[\n\\begin{aligned}\n\\text{P}(\\mu \\in [\\bar{X} - 2 \\sigma_{\\bar{X}}, \\bar{X} + 2 \\sigma_{\\bar{X}}]) &= \\text{P}(\\bar{X} -  2 \\sigma_{\\bar{X}} &lt; \\mu &lt; \\bar{X} +  2 \\sigma_{\\bar{X}}]) \\\\\n&= \\text{P}(-  2 \\sigma_{\\bar{X}} &lt; \\bar{X} - \\mu &lt; 2 \\sigma_{\\bar{X}})\\\\\n&= \\text{P}\\left(-2  &lt; \\frac{\\bar{X} - \\mu}{\\sigma_{\\bar{X}}} &lt; 2\\right)\\\\\n&= \\text{P}(-2 &lt; Z &lt; 2)\\\\\n&=0.95\n\\end{aligned}\n\\]\nSo our interval is \\(1100 \\pm 68\\)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Means</span>"
    ]
  },
  {
    "objectID": "means.html#confidence-intrval",
    "href": "means.html#confidence-intrval",
    "title": "5  Means",
    "section": "",
    "text": "We take a sample of 36 student SAT scores.\nWe observe a sample average of \\(\\bar{X} = 1100\\) and a sample standard deviation \\(s = 204\\)\nProvide an interval with 95% of containing the high school population average \\(\\mu\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Means</span>"
    ]
  },
  {
    "objectID": "means.html#t-test",
    "href": "means.html#t-test",
    "title": "5  Means",
    "section": "5.2 t-test",
    "text": "5.2 t-test\nWhen \\(N&lt;30\\) we can’t use CLT.\nSo what is the distribution of \\(\\bar{X}\\)?\nIf the population values are also approximately normal, as they are for SAT scores, then\n\\[\nt = \\frac{\\bar{X} - \\mu}{s/\\sqrt{N}}\n\\] Follows a t-distribution with \\(N-1\\) degrees of freedom.\n\n\n\n\n\n\nExample\n\n\n\nLet’s repeat the above example but \\(N=15\\)\nAll we have to do now is use the cutoff that gives us 0.95 for a t-distribution with 14 degrees of freedom.\nYou can use invT with area to the left and degrees of freedom.\nSo instead of 2 we use invT(0.975, 14) which is r qt(0.975, 14), a little bit bigger than 2.\nWe make our confidence interval\n\\[\n1100 \\pm 2.14 \\times 34\n\\]\nor\n\\[\n1100 \\pm 73\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Means</span>"
    ]
  },
  {
    "objectID": "means.html#difference-of-two-means",
    "href": "means.html#difference-of-two-means",
    "title": "5  Means",
    "section": "5.3 Difference of two means",
    "text": "5.3 Difference of two means\n\n5.3.1 Confidence interval\n\nAre the mean SAT scores in two high schools different?\nThe sample averages are \\(\\bar{X}_1 = 1200\\) and \\(\\bar{X}_2 = 1100\\) and the sample standard deviations are \\(s_1 = 200\\) and \\(s_2 = 180\\)\nThe sample sizes are \\(N_1=30\\) and \\(N_2=35\\)\nProvide a 95% confidence interval\n\nWe know the difference \\(\\bar{X}_1 - \\bar{X}_2\\) had the following mean and SD:\n\n\\(\\mu_{\\bar{X}_1 - \\bar{X}_2} = \\mu_1 - mu_2\\)\n\\(\\sigma_{\\bar{X}_1 - \\bar{X}_2} = \\sqrt{\\frac{\\sigma_1^2}{N_1} + \\frac{\\sigma_2^2}{N_2}}\\)\n\nTo construct a 95% confidence interval we use \\(\\bar{X}_1 - \\bar{X}_2 \\pm \\sigma_{\\bar{X}_1 - \\bar{X}_2}\\)\nWe approximate \\(\\sigma_1\\) and \\(\\sigma_2\\) with \\(s_1\\) and \\(s_2\\)\n\\[\n\\sigma_{\\bar{X}_1 - \\bar{X}_2} \\approx  \\sqrt{ \\frac{s_1^2}{N_1} + \\frac{s_2^2}{N_2}}\n\\]\nThe sample sizes are large enough that we can use CLT so the confidence interval is\n\\[\n\\bar{X}_1 - \\bar{X}_2 \\pm 2 \\sqrt{ \\frac{s_1^2}{N_1} + \\frac{s_2^2}{N_2}}\n\\]\n\\[\n\\bar{X}_1 - \\bar{X}_2 = 100\n\\]\nand\n\\[\n2 \\sqrt{ \\frac{s_1^2}{N_1} + \\frac{s_2^2}{N_2}} = 2 \\sqrt{ 200^2/30 + 180^2/35} \\approx 48\n\\]\nSo the confidence interval is\n\\[\n100 \\pm 48\n\\]\n\n\n5.3.2 p-value\nIs the difference we saw significant?\nWe already computed \\(\\sigma_{\\bar{X}_1 - \\bar{X_2}} \\approx 24\\)\n\\[\n\\text{P}(|\\bar{X}_1 - \\bar{X_2}| &gt; 100) = \\text{P}(|Z| &gt; 100/47) \\approx 0.03\n\\]\nWe reject.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Means</span>"
    ]
  }
]